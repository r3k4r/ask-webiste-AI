"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var src_exports = {};
__export(src_exports, {
  ChatOpenAI: () => import_openai2.ChatOpenAI,
  Database: () => Database,
  HistoryService: () => HistoryService,
  InternalUpstashError: () => InternalUpstashError,
  MODEL_NAME_WITH_PROVIDER_SPLITTER: () => MODEL_NAME_WITH_PROVIDER_SPLITTER,
  RAGChat: () => RAGChat,
  RateLimitService: () => RateLimitService,
  RatelimitUpstashError: () => RatelimitUpstashError,
  UpstashError: () => UpstashError,
  anthropic: () => anthropic,
  custom: () => custom,
  groq: () => groq,
  mistralai: () => mistralai,
  ollama: () => ollama,
  openai: () => openai,
  openrouter: () => openrouter,
  togetherai: () => togetherai,
  upstash: () => upstash
});
module.exports = __toCommonJS(src_exports);

// src/error/model.ts
var UpstashError = class extends Error {
  constructor(message) {
    super(message);
    this.name = "ModelError";
  }
};

// src/rag-chat.ts
var import_traceable3 = require("langsmith/traceable");

// src/config.ts
var import_redis = require("@upstash/redis");
var import_vector = require("@upstash/vector");

// src/constants.ts
var DEFAULT_CHAT_SESSION_ID = "upstash-rag-chat-session";
var DEFAULT_CHAT_RATELIMIT_SESSION_ID = "upstash-rag-chat-ratelimit-session";
var DEFAULT_SIMILARITY_THRESHOLD = 0.5;
var DEFAULT_TOP_K = 5;
var DEFAULT_HISTORY_TTL = 86400;
var DEFAULT_HISTORY_LENGTH = 5;
var MODEL_NAME_WITH_PROVIDER_SPLITTER = "_";
var DEFAULT_NAMESPACE = "";
var DEFAULT_PROMPT = ({ context, question, chatHistory }) => `You are a friendly AI assistant augmented with an Upstash Vector Store.
  To help you answer the questions, a context and/or chat history will be provided.
  Answer the question at the end using only the information available in the context or chat history, either one is ok.

  -------------
  Chat history:
  ${chatHistory}
  -------------
  Context:
  ${context}
  -------------

  Question: ${question}
  Helpful answer:`;
var DEFAULT_PROMPT_WITHOUT_RAG = ({ question, chatHistory }) => `You are a friendly AI assistant.
    To help you answer the questions, a chat history will be provided.
    Answer the question at the end.
    -------------
    Chat history:
    ${chatHistory}
    -------------
    Question: ${question}
    Helpful answer:`;

// src/models.ts
var import_openai = require("@langchain/openai");
var import_langsmith = require("langsmith");
var import_mistralai = require("@langchain/mistralai");
var import_anthropic = require("@langchain/anthropic");
var setupAnalytics = (analytics, providerApiKey, providerBaseUrl, provider) => {
  if (!analytics) return {};
  switch (analytics.name) {
    case "helicone": {
      switch (provider) {
        case "openai": {
          return {
            baseURL: "https://oai.helicone.ai/v1",
            defaultHeaders: {
              "Helicone-Auth": `Bearer ${analytics.token}`,
              Authorization: `Bearer ${providerApiKey}`
            }
          };
        }
        case "anthropic": {
          return {
            baseURL: "https://anthropic.helicone.ai",
            defaultHeaders: {
              "Helicone-Auth": `Bearer ${analytics.token}`,
              Authorization: `Bearer ${providerApiKey}`
            }
          };
        }
        case "upstash": {
          return {
            baseURL: "https://qstash.helicone.ai/llm/v1",
            defaultHeaders: {
              "Helicone-Auth": `Bearer ${analytics.token}`,
              Authorization: `Bearer ${providerApiKey}`
            }
          };
        }
        default: {
          return {
            baseURL: "https://gateway.helicone.ai",
            defaultHeaders: {
              "Helicone-Auth": `Bearer ${analytics.token}`,
              "Helicone-Target-Url": providerBaseUrl,
              Authorization: `Bearer ${providerApiKey}`
            }
          };
        }
      }
    }
    case "langsmith": {
      if (analytics.token !== void 0 && analytics.token != "") {
        const client = new import_langsmith.Client({
          apiKey: analytics.token,
          apiUrl: analytics.apiUrl ?? "https://api.smith.langchain.com"
        });
        global.globalTracer = client;
        return { client };
      }
      return { client: void 0 };
    }
    case "cloudflare": {
      return {
        baseURL: `https://gateway.ai.cloudflare.com/v1/${analytics.accountId}/${analytics.gatewayName}/openai`,
        defaultHeaders: {
          Authorization: `Bearer ${providerApiKey}`,
          "Content-Type": "application/json"
        }
      };
    }
    default: {
      throw new Error(`Unsupported analytics provider: ${JSON.stringify(analytics)}`);
    }
  }
};
var createLLMClient = (model, options, provider) => {
  const apiKey = options.apiKey ?? process.env.OPENAI_API_KEY ?? "";
  const organization = options.organization ?? process.env.OPENAI_ORGANIZATION ?? "";
  const providerBaseUrl = options.baseUrl;
  if (!apiKey) {
    throw new Error(
      "API key is required. Provide it in options or set OPENAI_API_KEY environment variable."
    );
  }
  const { analytics, ...restOptions } = options;
  const analyticsSetup = setupAnalytics(analytics, apiKey, providerBaseUrl, provider);
  return new import_openai.ChatOpenAI({
    modelName: model,
    streamUsage: provider !== "upstash",
    temperature: 0,
    ...restOptions,
    apiKey,
    configuration: {
      baseURL: analyticsSetup.baseURL ?? providerBaseUrl,
      ...analyticsSetup.defaultHeaders && { defaultHeaders: analyticsSetup.defaultHeaders },
      organization
    }
  });
};
var upstash = (model, options) => {
  const apiKey = options?.apiKey ?? process.env.QSTASH_TOKEN ?? "";
  if (!apiKey) {
    throw new Error(
      "Failed to create upstash LLM client: QSTASH_TOKEN not found. Pass apiKey parameter or set QSTASH_TOKEN env variable."
    );
  }
  return createLLMClient(
    model,
    { ...options, apiKey, baseUrl: "https://qstash.upstash.io/llm/v1" },
    "upstash"
  );
};
var custom = (model, options) => {
  if (!options.baseUrl) throw new Error("baseUrl cannot be empty or undefined.");
  return createLLMClient(model, options, "custom");
};
var openai = (model, options) => {
  return createLLMClient(model, { ...options, baseUrl: "https://api.openai.com/v1" }, "openai");
};
var DEFAULT_OLLAMA_PORT = 11434;
var ollama = (model, options) => {
  const baseUrl = `http://localhost:${options?.port ?? DEFAULT_OLLAMA_PORT}`;
  fetch(`${baseUrl}/api/tags`).then((response) => response.json()).then((data) => {
    const modelExists = data.models.some((m) => m.name.includes(model));
    if (!modelExists) {
      console.error(`Model not found. Please pull the model before running.
            Run: ollama pull ${model}`);
    }
  }).catch((error) => {
    console.error("Error checking model availability:", error);
  });
  return createLLMClient(model, { ...options, baseUrl: `${baseUrl}/v1` }, "ollama");
};
var groq = (model, options) => {
  return createLLMClient(model, { ...options, baseUrl: "https://api.groq.com/openai/v1" }, "groq");
};
var togetherai = (model, options) => {
  return createLLMClient(
    model,
    { ...options, baseUrl: "https://api.together.xyz/v1" },
    "togetherai"
  );
};
var openrouter = (model, options) => {
  return createLLMClient(
    model,
    { ...options, baseUrl: "https://openrouter.ai/api/v1" },
    "openrouter"
  );
};
var mistralai = (model, options) => {
  return new import_mistralai.ChatMistralAI({
    model,
    ...options
  });
};
var anthropic = (model, options) => {
  if (!options?.apiKey) {
    throw new Error("Failed to create Anthropic client: Anthropic key not found.");
  }
  const analyticsSetup = options.analytics ? setupAnalytics(options.analytics, options.apiKey, void 0, "anthropic") : void 0;
  return new import_anthropic.ChatAnthropic({
    model,
    ...options,
    clientOptions: {
      baseURL: analyticsSetup?.baseURL,
      defaultHeaders: analyticsSetup?.defaultHeaders
    }
  });
};

// src/config.ts
var Config = class {
  vector;
  redis;
  ratelimit;
  ratelimitSessionId;
  model;
  prompt;
  streaming;
  namespace;
  metadata;
  sessionId;
  debug;
  constructor(config) {
    this.redis = config?.redis ?? initializeRedis();
    this.ratelimit = config?.ratelimit;
    this.ratelimitSessionId = config?.ratelimitSessionId;
    this.streaming = config?.streaming;
    this.namespace = config?.namespace;
    this.metadata = config?.metadata;
    this.sessionId = config?.sessionId;
    this.model = config?.model ?? initializeModel();
    this.prompt = config?.promptFn ?? DEFAULT_PROMPT;
    this.vector = config?.vector ?? import_vector.Index.fromEnv();
    this.debug = config?.debug;
  }
};
var initializeRedis = () => {
  const environment = typeof process === "undefined" ? {} : process.env;
  return environment.UPSTASH_REDIS_REST_URL && environment.UPSTASH_REDIS_REST_TOKEN ? import_redis.Redis.fromEnv() : void 0;
};
var initializeModel = () => {
  const qstashToken = process.env.QSTASH_TOKEN;
  const openAIToken = process.env.OPENAI_API_KEY;
  const organization = process.env.OPENAI_ORGANIZATION;
  if (qstashToken) return upstash("meta-llama/Meta-Llama-3-8B-Instruct", { apiKey: qstashToken });
  if (openAIToken) {
    return openai("gpt-4o", { apiKey: openAIToken, organization });
  }
  throw new Error(
    "[RagChat Error]: Unable to connect to model. Pass one of OPENAI_API_KEY or QSTASH_TOKEN environment variables."
  );
};

// src/context-service/index.ts
var import_traceable = require("langsmith/traceable");
var import_nanoid = require("nanoid");

// src/utils.ts
var sanitizeQuestion = (question) => {
  return question.trim().replaceAll("\n", " ");
};
var formatFacts = (facts) => {
  return facts.join("\n");
};
function isOpenAIChatLanguageModel(model) {
  return Object.prototype.hasOwnProperty.call(model, "specificationVersion");
}

// src/context-service/index.ts
var ContextService = class {
  #vectorService;
  namespace;
  constructor(vectorService, namespace) {
    this.#vectorService = vectorService;
    this.namespace = namespace;
  }
  /**
   * A method that allows you to add various data types into a vector database.
   * It supports plain text, embeddings, PDF, and CSV. Additionally, it handles text-splitting for CSV and PDF.
   *
   * @example
   * ```typescript
   * await addDataToVectorDb({
   *   dataType: "pdf",
   *   fileSource: "./data/the_wonderful_wizard_of_oz.pdf",
   *   opts: { chunkSize: 500, chunkOverlap: 50 },
   * });
   * // OR
   * await addDataToVectorDb({
   *   dataType: "text",
   *   data: "Paris, the capital of France, is renowned for its iconic landmark, the Eiffel Tower, which was completed in 1889 and stands at 330 meters tall.",
   * });
   * ```
   */
  async add(args) {
    if (typeof args === "string") {
      const result = await this.#vectorService.save({
        type: "text",
        data: args,
        id: (0, import_nanoid.nanoid)(),
        options: { namespace: this.namespace }
      });
      return result;
    }
    return await this.#vectorService.save(args);
  }
  // eslint-disable-next-line @typescript-eslint/require-await
  async addMany(args) {
    return args.map((data) => this.add(data));
  }
  async deleteEntireContext(options) {
    await this.#vectorService.reset(
      options?.namespace ? { namespace: options.namespace } : void 0
    );
  }
  async delete({ id, namespace }) {
    await this.#vectorService.delete({ ids: typeof id === "string" ? [id] : id, namespace });
  }
  /** This is internal usage only. */
  _getContext(optionsWithDefault, input, debug) {
    return (0, import_traceable.traceable)(
      async (sessionId) => {
        await debug?.logSendPrompt(input);
        debug?.startRetrieveContext();
        if (optionsWithDefault.disableRAG) return { formattedContext: "", metadata: [] };
        const retrieveContext = (0, import_traceable.traceable)(
          async (payload) => {
            const originalContext = await this.#vectorService.retrieve(payload);
            const clonedContext = structuredClone(originalContext);
            return await optionsWithDefault.onContextFetched?.(clonedContext) ?? originalContext;
          },
          { name: "Step: Fetch", metadata: { sessionId }, run_type: "retriever" }
        );
        const context = await retrieveContext({
          question: input,
          similarityThreshold: optionsWithDefault.similarityThreshold,
          topK: optionsWithDefault.topK,
          namespace: optionsWithDefault.namespace
        });
        await debug?.endRetrieveContext(context);
        return {
          formattedContext: await (0, import_traceable.traceable)(
            (_context) => formatFacts(_context.map(({ data }) => data)),
            {
              name: "Step: Format",
              metadata: { sessionId },
              run_type: "tool"
            }
          )(context),
          metadata: context.map(({ metadata }) => metadata),
          rawContext: context
        };
      },
      {
        name: "Retrieve Context",
        metadata: {
          sessionId: optionsWithDefault.sessionId,
          namespace: optionsWithDefault.namespace
        }
      }
    );
  }
};

// src/database.ts
var import_nanoid3 = require("nanoid");

// src/file-loader.ts
var import_csv = require("@langchain/community/document_loaders/fs/csv");
var import_pdf = require("@langchain/community/document_loaders/fs/pdf");
var import_cheerio = require("@langchain/community/document_loaders/web/cheerio");
var import_html_to_text = require("@langchain/community/document_transformers/html_to_text");
var import_documents = require("@langchain/core/documents");
var import_text = require("langchain/document_loaders/fs/text");
var import_text_splitter = require("langchain/text_splitter");
var import_nanoid2 = require("nanoid");
var import_unstructured_client = require("unstructured-client");
var FileDataLoader = class {
  config;
  constructor(config) {
    this.config = config;
  }
  async loadFile(args) {
    const loader = this.createLoader(args);
    const _loader = await loader;
    const documents = await _loader.load();
    return (args2) => this.transformDocument(documents, args2);
  }
  async createLoader(args) {
    if (hasProcessor(this.config)) {
      return await this.createLoaderForProcessors();
    }
    switch (this.config.type) {
      case "pdf": {
        return new import_pdf.PDFLoader(
          this.config.fileSource,
          args
        );
      }
      case "csv": {
        return new import_csv.CSVLoader(
          this.config.fileSource,
          args
        );
      }
      case "text-file": {
        return new import_text.TextLoader(this.config.fileSource);
      }
      case "html": {
        return this.isURL(this.config.source) ? new import_cheerio.CheerioWebBaseLoader(this.config.source) : new import_text.TextLoader(this.config.source);
      }
      default: {
        throw new Error(`Unsupported data type: ${this.config.type}`);
      }
    }
  }
  async createLoaderForProcessors() {
    if (!hasProcessor(this.config)) throw new Error("Only processors are allowed");
    const client = new import_unstructured_client.UnstructuredClient({
      serverURL: "https://api.unstructuredapp.io",
      security: {
        apiKeyAuth: this.config.processor.options.apiKey
      }
    });
    const fileData = await Bun.file(this.config.fileSource).text();
    const response = await client.general.partition({
      //@ts-expect-error Will be fixed soon
      partitionParameters: {
        files: {
          content: fileData,
          //@ts-expect-error TS can't pick up the correct type due to complex union
          fileName: this.config.fileSource
        },
        ...this.config.processor.options
      }
    });
    const elements = response.elements?.filter(
      (element) => typeof element.text === "string"
    );
    return {
      // eslint-disable-next-line @typescript-eslint/require-await
      load: async () => {
        const documents = [];
        for (const element of elements) {
          const { metadata, text } = element;
          if (typeof text === "string" && text !== "") {
            documents.push(
              new import_documents.Document({
                pageContent: text,
                metadata: {
                  ...metadata,
                  category: element.type
                }
              })
            );
          }
        }
        return documents;
      }
    };
  }
  isURL(source) {
    return typeof source === "string" && source.startsWith("http");
  }
  async transformDocument(documents, args) {
    switch (this.config.type) {
      case "pdf": {
        const splitter = new import_text_splitter.RecursiveCharacterTextSplitter(args);
        const splittedDocuments = await splitter.splitDocuments(documents);
        return mapDocumentsIntoInsertPayload(splittedDocuments, (metadata, index) => ({
          source: metadata.source,
          timestamp: (/* @__PURE__ */ new Date()).toISOString(),
          paragraphNumber: index + 1,
          pageNumber: metadata.loc?.pageNumber || void 0,
          author: metadata.pdf?.info?.Author || void 0,
          title: metadata.pdf?.info?.Title || void 0,
          totalPages: metadata.pdf?.totalPages || void 0,
          language: metadata.pdf?.metadata?._metadata?.["dc:language"] || void 0
        }));
      }
      case "csv": {
        return mapDocumentsIntoInsertPayload(documents);
      }
      case "text-file": {
        const splitter = new import_text_splitter.RecursiveCharacterTextSplitter(args);
        const splittedDocuments = await splitter.splitDocuments(documents);
        return mapDocumentsIntoInsertPayload(splittedDocuments);
      }
      case "html": {
        const splitter = import_text_splitter.RecursiveCharacterTextSplitter.fromLanguage("html", args);
        const transformer = new import_html_to_text.HtmlToTextTransformer();
        const sequence = splitter.pipe(transformer);
        const newDocuments = await sequence.invoke(documents);
        return mapDocumentsIntoInsertPayload(newDocuments);
      }
      // Processors will be handled here. E.g. "unstructured", "llama-parse"
      case void 0: {
        const documents_ = documents.map(
          (item) => new import_documents.Document({ pageContent: item.pageContent, metadata: item.metadata })
        );
        return documents_.map((document) => ({
          data: document.pageContent,
          metadata: document.metadata,
          id: (0, import_nanoid2.nanoid)()
        }));
      }
      default: {
        throw new Error(`Unsupported data type: ${this.config.type}`);
      }
    }
    function mapDocumentsIntoInsertPayload(splittedDocuments, metadataMapper) {
      return splittedDocuments.map((document, index) => ({
        data: document.pageContent,
        id: (0, import_nanoid2.nanoid)(),
        ...metadataMapper ? { metadata: metadataMapper(document.metadata, index) } : {}
      }));
    }
  }
};
function hasProcessor(data) {
  return "processor" in data && typeof data.processor === "object" && "options" in data.processor;
}

// src/database.ts
var Database = class {
  index;
  constructor(index) {
    this.index = index;
  }
  async reset(options) {
    await this.index.reset({ namespace: options?.namespace });
  }
  async delete({ ids, namespace }) {
    await this.index.delete(ids, { namespace });
  }
  /**
   * A method that allows you to query the vector database with plain text.
   * It takes care of the text-to-embedding conversion by itself.
   * Additionally, it lets consumers pass various options to tweak the output.
   */
  // eslint-disable-next-line @typescript-eslint/no-unnecessary-type-parameters
  async retrieve({
    question,
    similarityThreshold = DEFAULT_SIMILARITY_THRESHOLD,
    topK = DEFAULT_TOP_K,
    namespace
  }) {
    const index = this.index;
    const result = await index.query(
      {
        data: question,
        topK,
        includeData: true,
        includeMetadata: true
      },
      { namespace }
    );
    const allValuesUndefined = result.every((embedding) => embedding.data === void 0);
    if (allValuesUndefined) {
      console.error("There is no answer for this question in the provided context.");
      return [
        {
          data: "There is no answer for this question in the provided context.",
          id: "error",
          metadata: {}
        }
      ];
    }
    const facts = result.filter((x) => x.score >= similarityThreshold).map((embedding) => ({
      data: embedding.data ?? "",
      id: embedding.id.toString(),
      metadata: embedding.metadata
    }));
    return facts;
  }
  /**
   * A method that allows you to add various data types into a vector database.
   * It supports plain text, embeddings, PDF, HTML, Text file and CSV. Additionally, it handles text-splitting for CSV, PDF and Text file.
   */
  async save(input) {
    const { namespace } = input.options ?? {};
    if (input.type === "text") {
      try {
        const vectorId = await this.index.upsert(
          {
            data: input.data,
            id: input.id ?? (0, import_nanoid3.nanoid)(),
            metadata: input.options?.metadata
          },
          { namespace }
        );
        return { success: true, ids: [vectorId.toString()] };
      } catch (error) {
        return { success: false, error: JSON.stringify(error, Object.getOwnPropertyNames(error)) };
      }
    } else if (input.type === "embedding") {
      try {
        const vectorId = await this.index.upsert(
          {
            vector: input.data,
            id: input.id ?? (0, import_nanoid3.nanoid)(),
            metadata: input.options?.metadata
          },
          { namespace }
        );
        return { success: true, ids: [vectorId.toString()] };
      } catch (error) {
        return { success: false, error: JSON.stringify(error, Object.getOwnPropertyNames(error)) };
      }
    } else {
      try {
        const fileArgs = "pdfConfig" in input ? input.pdfConfig : "csvConfig" in input ? input.csvConfig : {};
        const transformOrSplit = await new FileDataLoader(input).loadFile(fileArgs);
        const transformArgs = "config" in input ? input.config : {};
        const transformDocuments = await transformOrSplit(transformArgs);
        await this.index.upsert(transformDocuments, { namespace });
        return { success: true, ids: transformDocuments.map((document) => document.id) };
      } catch (error) {
        console.error(error);
        return { success: false, error: JSON.stringify(error, Object.getOwnPropertyNames(error)) };
      }
    }
  }
};

// src/error/ratelimit.ts
var RatelimitUpstashError = class extends Error {
  constructor(message, cause) {
    super(message);
    this.name = "RatelimitError";
    this.cause = cause;
  }
};

// src/error/internal.ts
var InternalUpstashError = class extends Error {
  constructor(message) {
    super(message);
    this.name = "InternalError";
  }
};

// src/error/vector.ts
var UpstashVectorError = class extends Error {
  constructor(message) {
    super(message);
    this.name = "VectorError";
  }
};

// src/history-service/redis-custom-history.ts
var import_redis2 = require("@upstash/redis");
var UpstashRedisHistory = class {
  client;
  constructor(_config) {
    const { config, client } = _config;
    if (client) {
      this.client = client;
    } else if (config) {
      this.client = new import_redis2.Redis(config);
    } else {
      throw new Error(
        `Upstash Redis message stores require either a config object or a pre-configured client.`
      );
    }
  }
  async addMessage({
    message,
    sessionId = DEFAULT_CHAT_SESSION_ID,
    sessionTTL
  }) {
    await this.client.lpush(sessionId, JSON.stringify(message));
    if (sessionTTL) {
      await this.client.expire(sessionId, sessionTTL);
    }
  }
  async deleteMessages({ sessionId }) {
    await this.client.del(sessionId);
  }
  async getMessages({
    sessionId = DEFAULT_CHAT_SESSION_ID,
    amount = DEFAULT_HISTORY_LENGTH,
    startIndex = 0
  }) {
    const endIndex = startIndex + amount - 1;
    const storedMessages = await this.client.lrange(
      sessionId,
      startIndex,
      endIndex
    );
    const orderedMessages = storedMessages.reverse();
    const messagesWithIndex = orderedMessages.map((message, index) => ({
      ...message,
      id: (startIndex + index).toString()
    }));
    return messagesWithIndex;
  }
};

// src/history-service/in-memory-history.ts
var InMemoryHistory = class {
  constructor() {
    if (!global.store) global.store = {};
  }
  async addMessage({
    message,
    sessionId = DEFAULT_CHAT_SESSION_ID,
    sessionTTL: _
  }) {
    if (!global.store[sessionId]) {
      global.store[sessionId] = { messages: [] };
    }
    const oldMessages = global.store[sessionId].messages || [];
    const newMessages = [
      {
        ...message
        // __internal_order: oldMessages.length,
      },
      ...oldMessages
    ];
    global.store[sessionId].messages = newMessages;
  }
  async deleteMessages({ sessionId }) {
    if (!global.store[sessionId]) {
      return;
    }
    global.store[sessionId].messages = [];
  }
  async getMessages({
    sessionId = DEFAULT_CHAT_SESSION_ID,
    amount = DEFAULT_HISTORY_LENGTH
  }) {
    if (!global.store[sessionId]) {
      global.store[sessionId] = { messages: [] };
    }
    const messages = global.store[sessionId]?.messages ?? [];
    const sortedMessages = messages.slice(0, amount).reverse();
    const messagesWithId = sortedMessages.map((message, index) => ({
      ...message,
      id: index.toString()
    }));
    return messagesWithId;
  }
};

// src/history-service/index.ts
var HistoryService = class {
  service;
  constructor(fields) {
    this.service = fields?.redis ? new UpstashRedisHistory({
      client: fields.redis
    }) : new InMemoryHistory();
  }
};

// src/llm-service.ts
var import_messages = require("@langchain/core/messages");
var import_ai = require("ai");
var import_traceable2 = require("langsmith/traceable");
var LLMService = class {
  constructor(model) {
    this.model = model;
  }
  callLLM(optionsWithDefault, _options, callbacks, debug) {
    return (0, import_traceable2.traceable)(
      (prompt) => {
        debug?.startLLMResponse();
        return optionsWithDefault.streaming ? this.makeStreamingLLMRequest(prompt, callbacks) : this.makeLLMRequest(prompt, callbacks.onComplete);
      },
      { name: "LLM Response", metadata: { sessionId: optionsWithDefault.sessionId } }
    );
  }
  async makeStreamingLLMRequest(prompt, {
    onComplete,
    onChunk
  }) {
    let stream;
    if (isOpenAIChatLanguageModel(this.model)) {
      const { textStream } = await (0, import_ai.streamText)({
        model: this.model,
        prompt
      });
      stream = textStream;
    } else {
      stream = await this.model.stream([
        new import_messages.HumanMessage(prompt)
      ]);
    }
    const reader = stream.getReader();
    let concatenatedOutput = "";
    const newStream = new ReadableStream({
      start(controller) {
        const processStream = async () => {
          let done;
          let value;
          try {
            while (true) {
              ({ done, value } = await reader.read());
              if (done) break;
              if (typeof value === "string") {
                controller.enqueue(value);
                continue;
              } else {
                const message = value?.content ?? "";
                onChunk?.({
                  content: message,
                  inputTokens: value?.usage_metadata?.input_tokens ?? 0,
                  chunkTokens: value?.usage_metadata?.output_tokens ?? 0,
                  totalTokens: value?.usage_metadata?.total_tokens ?? 0,
                  rawContent: value
                });
                concatenatedOutput += message;
                controller.enqueue(message);
              }
            }
            controller.close();
            onComplete?.(concatenatedOutput);
          } catch (error) {
            controller.error(error);
          }
        };
        void processStream();
      }
    });
    return { output: newStream, isStream: true };
  }
  async makeLLMRequest(prompt, onComplete) {
    let content;
    if (isOpenAIChatLanguageModel(this.model)) {
      const { text } = await (0, import_ai.generateText)({
        model: this.model,
        prompt
      });
      content = text;
    } else {
      const { content: output } = await this.model.invoke(prompt);
      content = output;
    }
    onComplete?.(content);
    return { output: content, isStream: false };
  }
};

// src/logger.ts
var ChatLogger = class {
  logs = [];
  options;
  eventStartTimes = /* @__PURE__ */ new Map();
  constructor(options) {
    this.options = options;
  }
  async log(level, eventType, details, latency) {
    if (this.shouldLog(level)) {
      const timestamp = Date.now();
      const logEntry = {
        timestamp,
        logLevel: level,
        eventType,
        details,
        latency
      };
      this.logs.push(logEntry);
      if (this.options.logOutput === "console") {
        await this.writeToConsole(logEntry);
      }
      await new Promise((resolve) => setTimeout(resolve, 100));
    }
  }
  // eslint-disable-next-line @typescript-eslint/require-await
  async writeToConsole(logEntry) {
    const JSON_SPACING = 2;
    console.log(
      JSON.stringify(
        {
          ...logEntry,
          ...logEntry.latency && logEntry.latency > 0 ? { latency: `${logEntry.latency}ms` } : void 0
        },
        void 0,
        JSON_SPACING
      )
    );
  }
  shouldLog(level) {
    const levels = ["DEBUG", "INFO", "WARN", "ERROR"];
    return levels.indexOf(level) >= levels.indexOf(this.options.logLevel);
  }
  startTimer(eventType) {
    this.eventStartTimes.set(eventType, Date.now());
  }
  endTimer(eventType) {
    const startTime = this.eventStartTimes.get(eventType);
    if (startTime) {
      this.eventStartTimes.delete(eventType);
      return Date.now() - startTime;
    }
    return void 0;
  }
  async logSendPrompt(prompt) {
    await this.log("INFO", "SEND_PROMPT", { prompt });
  }
  startRetrieveHistory() {
    this.startTimer("RETRIEVE_HISTORY");
  }
  async endRetrieveHistory(history) {
    const latency = this.endTimer("RETRIEVE_HISTORY");
    await this.log("INFO", "RETRIEVE_HISTORY", { history }, latency);
  }
  startRetrieveContext() {
    this.startTimer("RETRIEVE_CONTEXT");
  }
  async endRetrieveContext(context) {
    const latency = this.endTimer("RETRIEVE_CONTEXT");
    await this.log("INFO", "RETRIEVE_CONTEXT", { context }, latency);
  }
  async logRetrieveFormatHistory(formattedHistory) {
    await this.log("INFO", "FORMAT_HISTORY", { formattedHistory });
  }
  async logFinalPrompt(prompt) {
    await this.log("INFO", "FINAL_PROMPT", { prompt });
  }
  startLLMResponse() {
    this.startTimer("LLM_RESPONSE");
  }
  async endLLMResponse(response) {
    const latency = this.endTimer("LLM_RESPONSE");
    await this.log("INFO", "LLM_RESPONSE", { response }, latency);
  }
  async logError(error) {
    await this.log("ERROR", "ERROR", { message: error.message, stack: error.stack });
  }
  getLogs() {
    return this.logs;
  }
};

// src/ratelimit-service.ts
var RateLimitService = class {
  ratelimit;
  constructor(ratelimit) {
    this.ratelimit = ratelimit;
  }
  async checkLimit(sessionId) {
    if (!this.ratelimit) {
      return {
        success: true,
        limit: -1,
        remaining: -1,
        pending: Promise.resolve(),
        reset: -1
      };
    }
    const result = await this.ratelimit.limit(sessionId);
    return {
      success: result.success,
      remaining: result.remaining,
      reset: result.reset,
      limit: result.limit,
      pending: result.pending,
      reason: result.reason
    };
  }
};

// src/rag-chat.ts
var RAGChat = class {
  ratelimit;
  llm;
  context;
  history;
  config;
  debug;
  constructor(config) {
    this.config = new Config(config);
    if (!this.config.vector) {
      throw new UpstashVectorError("Vector can not be undefined!");
    }
    if (!this.config.model) {
      throw new UpstashError("Model can not be undefined!");
    }
    const vectorService = new Database(this.config.vector);
    this.history = new HistoryService({
      redis: this.config.redis
    }).service;
    this.llm = new LLMService(this.config.model);
    this.context = new ContextService(vectorService, this.config.namespace ?? DEFAULT_NAMESPACE);
    this.debug = this.config.debug ? new ChatLogger({
      logLevel: "INFO",
      logOutput: "console"
    }) : void 0;
    this.ratelimit = new RateLimitService(this.config.ratelimit);
  }
  /**
   * A method that allows you to chat LLM using Vector DB as your knowledge store and Redis - optional - as a chat history.
   *
   * @example
   * ```typescript
   *await ragChat.chat("Where is the capital of Turkey?", {
   *  stream: false,
   *})
   * ```
   */
  async chat(input, options) {
    return (0, import_traceable3.traceable)(
      async (input2, options2) => {
        try {
          const optionsWithDefault = this.getOptionsWithDefaults(options2);
          await this.checkRatelimit(optionsWithDefault);
          if (!optionsWithDefault.disableHistory) {
            await this.addUserMessageToHistory(input2, optionsWithDefault);
          }
          const question = sanitizeQuestion(input2);
          const {
            formattedContext: context,
            metadata,
            rawContext
          } = await this.context._getContext(
            optionsWithDefault,
            input2,
            this.debug
          )(optionsWithDefault.sessionId);
          const { formattedHistory, modifiedChatHistory } = await this.getChatHistory(optionsWithDefault);
          const prompt = await this.generatePrompt(
            optionsWithDefault,
            context,
            question,
            formattedHistory
          );
          const llmResult = await this.llm.callLLM(
            optionsWithDefault,
            options2,
            {
              onChunk: optionsWithDefault.onChunk,
              onComplete: async (output) => {
                await this.debug?.endLLMResponse(output);
                if (!optionsWithDefault.disableHistory) {
                  await this.addAssistantMessageToHistory(output, optionsWithDefault);
                }
              }
            },
            this.debug
          )(prompt);
          return {
            ...llmResult,
            metadata,
            context: rawContext ?? [],
            history: modifiedChatHistory
          };
        } catch (error) {
          await this.debug?.logError(error);
          throw error;
        }
      },
      {
        name: "Rag Chat",
        ...global.globalTracer === void 0 ? { tracingEnabled: false, client: void 0 } : { client: global.globalTracer, tracingEnabled: true },
        project_name: "Upstash Rag Chat",
        tags: [options?.streaming ? "streaming" : "non-streaming"],
        metadata: this.getOptionsWithDefaults(options)
      }
    )(input, options);
  }
  generatePrompt(optionsWithDefault, context, question, formattedHistory) {
    return (0, import_traceable3.traceable)(
      async (optionsWithDefault2, context2, question2, formattedHistory2) => {
        const prompt = optionsWithDefault2.promptFn({
          context: context2,
          question: question2,
          chatHistory: formattedHistory2
        });
        await this.debug?.logFinalPrompt(prompt);
        return prompt;
      },
      { name: "Final Prompt", run_type: "prompt" }
    )(optionsWithDefault, context, question, formattedHistory);
  }
  async getChatHistory(_optionsWithDefault) {
    return (0, import_traceable3.traceable)(
      async (optionsWithDefault) => {
        if (optionsWithDefault.disableHistory) {
          await this.debug?.logRetrieveFormatHistory("History disabled, returning empty history");
          return { formattedHistory: "", modifiedChatHistory: [] };
        }
        this.debug?.startRetrieveHistory();
        const originalChatHistory = await this.history.getMessages({
          sessionId: optionsWithDefault.sessionId,
          amount: optionsWithDefault.historyLength
        });
        const clonedChatHistory = structuredClone(originalChatHistory);
        const modifiedChatHistory = await optionsWithDefault.onChatHistoryFetched?.(clonedChatHistory) ?? originalChatHistory;
        await this.debug?.endRetrieveHistory(modifiedChatHistory);
        const formattedHistory = modifiedChatHistory.map((message) => {
          return message.role === "user" ? `USER MESSAGE: ${message.content}` : `YOUR MESSAGE: ${message.content}`;
        }).join("\n");
        await this.debug?.logRetrieveFormatHistory(formattedHistory);
        return { formattedHistory, modifiedChatHistory };
      },
      {
        name: "Retrieve History",
        tags: [this.config.redis === void 0 ? "in-memory" : "redis"],
        metadata: { sessionId: _optionsWithDefault.sessionId },
        run_type: "retriever"
      }
    )(_optionsWithDefault);
  }
  async addUserMessageToHistory(input, optionsWithDefault) {
    await this.history.addMessage({
      message: { content: input, role: "user" },
      sessionId: optionsWithDefault.sessionId
    });
  }
  async addAssistantMessageToHistory(output, optionsWithDefault) {
    await this.history.addMessage({
      message: {
        content: output,
        metadata: optionsWithDefault.metadata,
        role: "assistant"
      },
      sessionId: optionsWithDefault.sessionId
    });
  }
  async checkRatelimit(optionsWithDefault) {
    const ratelimitResponse = await this.ratelimit.checkLimit(
      optionsWithDefault.ratelimitSessionId
    );
    optionsWithDefault.ratelimitDetails?.(ratelimitResponse);
    if (!ratelimitResponse.success) {
      throw new RatelimitUpstashError("Couldn't process chat due to ratelimit.", {
        error: "ERR:USER_RATELIMITED",
        resetTime: ratelimitResponse.reset
      });
    }
  }
  getOptionsWithDefaults(options) {
    const isRagDisabledAndPromptFunctionMissing = options?.disableRAG && !options.promptFn;
    return {
      onChatHistoryFetched: options?.onChatHistoryFetched,
      onContextFetched: options?.onContextFetched,
      onChunk: options?.onChunk,
      ratelimitDetails: options?.ratelimitDetails,
      metadata: options?.metadata ?? this.config.metadata,
      namespace: options?.namespace ?? this.config.namespace ?? DEFAULT_NAMESPACE,
      streaming: options?.streaming ?? this.config.streaming ?? false,
      sessionId: options?.sessionId ?? this.config.sessionId ?? DEFAULT_CHAT_SESSION_ID,
      disableRAG: options?.disableRAG ?? false,
      disableHistory: options?.disableHistory ?? false,
      similarityThreshold: options?.similarityThreshold ?? DEFAULT_SIMILARITY_THRESHOLD,
      topK: options?.topK ?? DEFAULT_TOP_K,
      historyLength: options?.historyLength ?? DEFAULT_HISTORY_LENGTH,
      historyTTL: options?.historyTTL ?? DEFAULT_HISTORY_TTL,
      ratelimitSessionId: options?.ratelimitSessionId ?? this.config.ratelimitSessionId ?? DEFAULT_CHAT_RATELIMIT_SESSION_ID,
      promptFn: isRagDisabledAndPromptFunctionMissing ? DEFAULT_PROMPT_WITHOUT_RAG : options?.promptFn ?? this.config.prompt
    };
  }
};

// src/index.ts
var import_openai2 = require("@langchain/openai");
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  ChatOpenAI,
  Database,
  HistoryService,
  InternalUpstashError,
  MODEL_NAME_WITH_PROVIDER_SPLITTER,
  RAGChat,
  RateLimitService,
  RatelimitUpstashError,
  UpstashError,
  anthropic,
  custom,
  groq,
  mistralai,
  ollama,
  openai,
  openrouter,
  togetherai,
  upstash
});
