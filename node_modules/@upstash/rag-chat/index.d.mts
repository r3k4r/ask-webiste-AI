import * as langsmith_traceable from 'langsmith/traceable';
import { WebBaseLoaderParams } from '@langchain/community/document_loaders/web/cheerio';
import { Index } from '@upstash/vector';
import { RecursiveCharacterTextSplitterParams } from 'langchain/text_splitter';
import * as _langchain_openai from '@langchain/openai';
import { ChatOpenAI, OpenAIChatInput } from '@langchain/openai';
export { ChatOpenAI } from '@langchain/openai';
import { openai as openai$1 } from '@ai-sdk/openai';
import { Ratelimit } from '@upstash/ratelimit';
import { Redis, RedisConfigNodejs } from '@upstash/redis';
import * as _langchain_mistralai from '@langchain/mistralai';
import { ChatMistralAI } from '@langchain/mistralai';
import { ChatAnthropic } from '@langchain/anthropic';
import { UnstructuredLoaderOptions } from '@langchain/community/document_loaders/fs/unstructured';
import { Client } from 'langsmith';

declare const __brand: unique symbol;
type Brand<B> = {
    [__brand]: B;
};
type Branded<T, B> = T & Brand<B>;
type OptionalAsync<T> = T | Promise<T>;
type ChatOptions = {
    /** Length of the conversation history to include in your LLM query. Increasing this may lead to hallucinations. Retrieves the last N messages.
     * @default 5
     */
    historyLength?: number;
    /** Configuration to retain chat history. After the specified time, the history will be automatically cleared.
     * @default 86_400 // 1 day in seconds
     */
    historyTTL?: number;
    /** Configuration to adjust the accuracy of results.
     * @default 0.5
     */
    similarityThreshold?: number;
    /** Amount of data points to include in your LLM query.
     * @default 5
     */
    topK?: number;
    /**
     *  Details of applied rate limit.
     */
    ratelimitDetails?: (response: Awaited<ReturnType<Ratelimit["limit"]>>) => void;
    /**
     * Hook to modify or get data and details of each chunk. Can be used to alter streamed content.
     */
    onChunk?: ({ content, inputTokens, chunkTokens, totalTokens, rawContent, }: {
        inputTokens: number;
        chunkTokens: number;
        totalTokens: number;
        content: string;
        rawContent: string;
    }) => void;
    /**
     * Hook to access the retrieved context and modify as you wish.
     */
    onContextFetched?: (context: PrepareChatResult) => OptionalAsync<PrepareChatResult> | OptionalAsync<undefined | null>;
    /**
     * Hook to access the retrieved history and modify as you wish.
     */
    onChatHistoryFetched?: (messages: UpstashMessage[]) => OptionalAsync<UpstashMessage[]> | OptionalAsync<undefined | null>;
    /**
     * Allows disabling RAG and use chat as LLM in combination with prompt. This will give you ability to build your own pipelines.
     */
    disableRAG?: boolean;
    /**
     * Disables recording of the conversation in the chat history.
     * @default false
     */
    disableHistory?: boolean;
} & CommonChatAndRAGOptions;
type PrepareChatResult = {
    data: string;
    id: string;
    metadata: unknown;
}[];
/**Config needed to initialize RAG Chat SDK */
type RAGChatConfig = {
    vector?: Index;
    redis?: Redis;
    /**Any valid Langchain compatiable LLM will work
     * @example new ChatOpenAI({
        modelName: "gpt-3.5-turbo",
        streaming: true,
        verbose,
        temperature: 0,
        apiKey,
      })
    */
    model?: ChatOpenAI | ChatMistralAI | ChatAnthropic | OpenAIChatLanguageModel;
    /**
       * Ratelimit instance
       * @example new Ratelimit({
            redis,
            limiter: Ratelimit.tokenBucket(10, "1d", 10),
            prefix: "@upstash/rag-chat-ratelimit",
            })
       */
    ratelimit?: Ratelimit;
    /**
     * Logs every step of the chat, including sending prompts, listing history entries,
     * retrieving context from the vector database, and capturing the full response
     * from the LLM, including latency.
     */
    debug?: boolean;
} & CommonChatAndRAGOptions;
type AddContextOptions = {
    /**
     * Namespace of the index you wanted to insert. Default is empty string.
     * @default ""
     */
    metadata?: UpstashDict;
    namespace?: string;
};
type CommonChatAndRAGOptions = {
    /** Set to `true` if working with web apps and you want to be interactive without stalling users.
     */
    streaming?: true | false;
    /** Chat session ID of the user interacting with the application.
     * @default "upstash-rag-chat-session"
     */
    sessionId?: string;
    /**
     * Namespace of the index you wanted to query.
     */
    namespace?: string;
    /**
     * Metadata for your chat message. This could be used to store anything in the chat history. By default RAG Chat SDK uses this to persist used model name in the history
     */
    metadata?: UpstashDict;
    /** Rate limit session ID of the user interacting with the application.
     * @default "upstash-rag-chat-ratelimit-session"
     */
    ratelimitSessionId?: string;
    /**
       * If no Index name or instance is provided, falls back to the default.
       * @default
            PromptTemplate.fromTemplate(`You are a friendly AI assistant augmented with an Upstash Vector Store.
            To help you answer the questions, a context will be provided. This context is generated by querying the vector store with the user question.
            Answer the question at the end using only the information available in the context and chat history.
            If the answer is not available in the chat history or context, do not answer the question and politely let the user know that you can only answer if the answer is available in context or the chat history.
  
            -------------
            Chat history:
            {chat_history}
            -------------
            Context:
            {context}
            -------------
  
            Question: {question}
            Helpful answer:`)
       */
    promptFn?: CustomPrompt;
};
type HistoryOptions = Pick<ChatOptions, "historyLength" | "sessionId">;
type UpstashDict = Record<string, unknown>;
type UpstashMessage<TMetadata extends UpstashDict = UpstashDict> = {
    role: "assistant" | "user";
    content: string;
    metadata?: TMetadata | undefined;
    usage_metadata?: {
        input_tokens: number;
        output_tokens: number;
        total_tokens: number;
    };
    id: string;
};
type OpenAIChatLanguageModel = ReturnType<typeof openai$1>;

type FilePath = string;
type URL = string;
type ProcessorType = {
    name: "unstructured";
    options: UnstructuredLoaderOptions;
};
type DatasWithFileSource = {
    type?: "pdf" | "csv" | "text-file" | "html";
    fileSource: FilePath;
    options?: AddContextOptions;
    processor: ProcessorType;
} | {
    type: "pdf";
    fileSource: FilePath | Blob;
    options?: AddContextOptions;
    config?: Partial<RecursiveCharacterTextSplitterParams>;
    pdfConfig?: {
        parsedItemSeparator?: string;
        splitPages?: boolean;
    };
} | {
    type: "csv";
    fileSource: FilePath | Blob;
    options?: AddContextOptions;
    csvConfig?: {
        column?: string;
        separator?: string;
    };
} | {
    type: "text-file";
    fileSource: FilePath | Blob;
    options?: AddContextOptions;
    config?: Partial<RecursiveCharacterTextSplitterParams>;
} | ({
    type: "html";
    source: URL;
    htmlConfig?: WebBaseLoaderParams;
    options?: AddContextOptions;
    config: Partial<RecursiveCharacterTextSplitterParams>;
} | {
    type: "html";
    source: FilePath | Blob;
    options?: AddContextOptions;
    config?: Partial<RecursiveCharacterTextSplitterParams>;
});
type AddContextPayload = {
    type: "text";
    data: string;
    options?: AddContextOptions;
    id?: string | number;
} | {
    type: "embedding";
    data: number[];
    options?: AddContextOptions;
    id?: string | number;
} | DatasWithFileSource;
type VectorPayload = {
    question: string;
    similarityThreshold?: number;
    topK?: number;
    namespace?: string;
};
type ResetOptions = {
    namespace: string;
};
type SaveOperationResult = {
    success: true;
    ids: string[];
} | {
    success: false;
    error: string;
};
declare class Database {
    private index;
    constructor(index: Index);
    reset(options?: ResetOptions): Promise<void>;
    delete({ ids, namespace }: {
        ids: string[];
        namespace?: string;
    }): Promise<void>;
    /**
     * A method that allows you to query the vector database with plain text.
     * It takes care of the text-to-embedding conversion by itself.
     * Additionally, it lets consumers pass various options to tweak the output.
     */
    retrieve<TMetadata>({ question, similarityThreshold, topK, namespace, }: VectorPayload): Promise<{
        data: string;
        id: string;
        metadata: TMetadata;
    }[]>;
    /**
     * A method that allows you to add various data types into a vector database.
     * It supports plain text, embeddings, PDF, HTML, Text file and CSV. Additionally, it handles text-splitting for CSV, PDF and Text file.
     */
    save(input: AddContextPayload): Promise<SaveOperationResult>;
}

declare const _LOG_LEVELS: readonly ["DEBUG", "INFO", "WARN", "ERROR"];
type LogLevel = (typeof _LOG_LEVELS)[number];
type ChatLogEntry = {
    timestamp: number;
    logLevel: LogLevel;
    eventType: "SEND_PROMPT" | "RETRIEVE_HISTORY" | "RETRIEVE_CONTEXT" | "FINAL_PROMPT" | "FORMAT_HISTORY" | "LLM_RESPONSE" | "ERROR";
    details: unknown;
    latency?: number;
};
type ChatLoggerOptions = {
    logLevel: LogLevel;
    logOutput: "console";
};
declare class ChatLogger {
    private logs;
    private options;
    private eventStartTimes;
    constructor(options: ChatLoggerOptions);
    private log;
    private writeToConsole;
    private shouldLog;
    private startTimer;
    private endTimer;
    logSendPrompt(prompt: string): Promise<void>;
    startRetrieveHistory(): void;
    endRetrieveHistory(history: unknown[]): Promise<void>;
    startRetrieveContext(): void;
    endRetrieveContext(context: unknown): Promise<void>;
    logRetrieveFormatHistory(formattedHistory: unknown): Promise<void>;
    logFinalPrompt(prompt: unknown): Promise<void>;
    startLLMResponse(): void;
    endLLMResponse(response: unknown): Promise<void>;
    logError(error: Error): Promise<void>;
    getLogs(): ChatLogEntry[];
}

type DefaultChatOptions = {
    streaming: boolean;
    disableRAG: boolean;
    disableHistory: boolean;
    sessionId: string;
    ratelimitSessionId: string;
    similarityThreshold: number;
    topK: number;
    historyLength: number;
    historyTTL: number;
    namespace: string;
    promptFn: CustomPrompt;
};
type Modify<T, R> = Omit<T, keyof R> & R;
type ModifiedChatOptions = Modify<ChatOptions, DefaultChatOptions>;

declare class ContextService {
    #private;
    private readonly namespace;
    constructor(vectorService: Database, namespace: string);
    /**
     * A method that allows you to add various data types into a vector database.
     * It supports plain text, embeddings, PDF, and CSV. Additionally, it handles text-splitting for CSV and PDF.
     *
     * @example
     * ```typescript
     * await addDataToVectorDb({
     *   dataType: "pdf",
     *   fileSource: "./data/the_wonderful_wizard_of_oz.pdf",
     *   opts: { chunkSize: 500, chunkOverlap: 50 },
     * });
     * // OR
     * await addDataToVectorDb({
     *   dataType: "text",
     *   data: "Paris, the capital of France, is renowned for its iconic landmark, the Eiffel Tower, which was completed in 1889 and stands at 330 meters tall.",
     * });
     * ```
     */
    add(args: AddContextPayload | string): Promise<{
        success: true;
        ids: string[];
    } | {
        success: false;
        error: string;
    }>;
    addMany(args: AddContextPayload[] | string[]): Promise<Promise<{
        success: true;
        ids: string[];
    } | {
        success: false;
        error: string;
    }>[]>;
    deleteEntireContext(options?: ResetOptions): Promise<void>;
    delete({ id, namespace }: {
        id: string | string[];
        namespace?: string;
    }): Promise<void>;
    /** This is internal usage only. */
    _getContext<TMetadata extends object>(optionsWithDefault: ModifiedChatOptions, input: string, debug?: ChatLogger): langsmith_traceable.TraceableFunction<(sessionId: string) => Promise<{
        formattedContext: string;
        metadata: never[];
        rawContext?: undefined;
    } | {
        formattedContext: string;
        metadata: TMetadata[];
        rawContext: PrepareChatResult;
    }>>;
}

type HistoryAddMessage = {
    message: Omit<UpstashMessage, "id">;
    sessionId?: string;
    sessionTTL?: number;
};
declare abstract class BaseMessageHistory {
    abstract getMessages({ sessionId, amount, }: {
        sessionId: string;
        amount: number;
    }): Promise<UpstashMessage[]>;
    abstract addMessage(data: HistoryAddMessage): Promise<void>;
    abstract deleteMessages({ sessionId }: {
        sessionId: string;
    }): Promise<void>;
}

declare global {
    var store: Record<string, {
        messages: Omit<UpstashMessage, "id">[];
    }>;
}
declare class InMemoryHistory implements BaseMessageHistory {
    constructor();
    addMessage({ message, sessionId, sessionTTL: _, }: HistoryAddMessage): Promise<void>;
    deleteMessages({ sessionId }: {
        sessionId: string;
    }): Promise<void>;
    getMessages({ sessionId, amount, }: {
        sessionId?: string | undefined;
        amount?: number | undefined;
    }): Promise<UpstashMessage[]>;
}

type UpstashRedisHistoryConfig = {
    config?: RedisConfigNodejs;
    client?: Redis;
};
declare class UpstashRedisHistory implements BaseMessageHistory {
    client: Redis;
    constructor(_config: UpstashRedisHistoryConfig);
    addMessage({ message, sessionId, sessionTTL, }: HistoryAddMessage): Promise<void>;
    deleteMessages({ sessionId }: {
        sessionId: string;
    }): Promise<void>;
    getMessages({ sessionId, amount, startIndex, }: {
        sessionId?: string | undefined;
        amount?: number | undefined;
        startIndex?: number | undefined;
    }): Promise<UpstashMessage[]>;
}

type PromptParameters = {
    chatHistory?: string;
    question: string;
    context: string;
};
type CustomPrompt = ({ question, chatHistory, context }: PromptParameters) => string;
type Message = {
    id: string;
    content: string;
    role: "ai" | "user";
};
type ChatReturnType<TMetadata extends unknown[], T extends Partial<ChatOptions>> = Promise<(T["streaming"] extends true ? {
    output: ReadableStream<string>;
    isStream: true;
} : {
    output: string;
    isStream: false;
}) & {
    metadata: TMetadata;
    context: PrepareChatResult;
    history: UpstashMessage[];
}>;
declare class RAGChat {
    private ratelimit;
    private llm;
    context: ContextService;
    history: UpstashRedisHistory | InMemoryHistory;
    private config;
    private debug?;
    constructor(config?: RAGChatConfig);
    /**
     * A method that allows you to chat LLM using Vector DB as your knowledge store and Redis - optional - as a chat history.
     *
     * @example
     * ```typescript
     *await ragChat.chat("Where is the capital of Turkey?", {
     *  stream: false,
     *})
     * ```
     */
    chat<TMetadata extends object, TChatOptions extends ChatOptions = ChatOptions>(input: string, options?: TChatOptions): Promise<ChatReturnType<TMetadata[], TChatOptions>>;
    private generatePrompt;
    private getChatHistory;
    private addUserMessageToHistory;
    private addAssistantMessageToHistory;
    private checkRatelimit;
    private getOptionsWithDefaults;
}

type HistoryConfig = {
    redis?: Redis;
};
type GetHistoryOptions = {
    sessionId: string;
    length?: number;
    sessionTTL?: number;
};
declare class HistoryService {
    service: UpstashRedisHistory | InMemoryHistory;
    constructor(fields?: HistoryConfig);
}

declare class RateLimitService {
    private ratelimit?;
    constructor(ratelimit?: Ratelimit);
    checkLimit(sessionId: string): Promise<{
        success: boolean;
        limit: number;
        remaining: number;
        pending: Promise<void>;
        reset: number;
        reason?: undefined;
    } | {
        success: boolean;
        remaining: number;
        reset: number;
        limit: number;
        pending: Promise<unknown>;
        reason: ("timeout" | "cacheBlock" | "denyList") | undefined;
    }>;
}

declare const RATELIMIT_ERROR_MESSAGE = "ERR:USER_RATELIMITED";
declare const MODEL_NAME_WITH_PROVIDER_SPLITTER = "_";
declare const OLLAMA_MODELS: readonly ["llama3.1", "gemma2", "mistral-nemo", "mistral-large", "qwen2", "deepseek-coder-v2", "phi3", "mistral", "mixtral", "codegemma", "command-r", "command-r-plus", "llava", "llama3", "gemma", "qwen", "llama2", "codellama", "dolphin-mixtral", "nomic-embed-text", "llama2-uncensored", "phi", "deepseek-coder", "zephyr", "mxbai-embed-large", "dolphin-mistral", "orca-mini", "dolphin-llama3", "starcoder2", "yi", "mistral-openorca", "llama2-chinese", "llava-llama3", "starcoder", "vicuna", "tinyllama", "codestral", "wizard-vicuna-uncensored", "nous-hermes2", "wizardlm2", "openchat", "aya", "tinydolphin", "stable-code", "wizardcoder", "openhermes", "all-minilm", "granite-code", "codeqwen", "stablelm2", "wizard-math", "neural-chat", "phind-codellama", "llama3-gradient", "dolphincoder", "nous-hermes", "sqlcoder", "xwinlm", "deepseek-llm", "yarn-llama2", "llama3-chatqa", "starling-lm", "wizardlm", "falcon", "orca2", "snowflake-arctic-embed", "solar", "samantha-mistral", "moondream", "stable-beluga", "dolphin-phi", "bakllava", "deepseek-v2", "wizardlm-uncensored", "yarn-mistral", "medllama2", "llama-pro", "glm4", "nous-hermes2-mixtral", "meditron", "codegeex4", "nexusraven", "llava-phi3", "codeup", "everythinglm", "magicoder", "stablelm-zephyr", "codebooga", "mistrallite", "wizard-vicuna", "duckdb-nsql", "megadolphin", "falcon2", "notux", "goliath", "open-orca-platypus2", "notus", "internlm2", "llama3-groq-tool-use", "dbrx", "alfred", "mathstral", "firefunction-v2", "nuextract", "bge-m3", "bge-large", "paraphrase-multilingual"];

type RatelimitResponse = {
    error: typeof RATELIMIT_ERROR_MESSAGE;
    resetTime?: number;
};
declare class RatelimitUpstashError extends Error {
    constructor(message: string, cause: RatelimitResponse);
}

declare class UpstashError extends Error {
    constructor(message: string);
}

declare class InternalUpstashError extends Error {
    constructor(message: string);
}

declare global {
    /** Langsmith tracer to trace actions in RAG Chat, its initialized in `model.ts`. */
    var globalTracer: Client | undefined;
}
type OpenAIChatModel = "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4o" | "gpt-4o-mini" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0301" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613";
type UpstashChatModel = "mistralai/Mistral-7B-Instruct-v0.2" | "meta-llama/Meta-Llama-3-8B-Instruct";
type LLMClientConfig = {
    temperature?: number;
    topP?: number;
    frequencyPenalty?: number;
    presencePenalty?: number;
    n?: number;
    logitBias?: Record<string, number>;
    model: string;
    modelKwargs?: OpenAIChatInput["modelKwargs"];
    stop?: string[];
    stopSequences?: string[];
    user?: string;
    timeout?: number;
    streamUsage?: boolean;
    maxTokens?: number;
    logprobs?: boolean;
    topLogprobs?: number;
    openAIApiKey?: string;
    organization?: string;
    apiKey?: string;
    baseUrl: string;
};
type AnalyticsConfig = {
    name: "helicone";
    token: string;
} | {
    name: "langsmith";
    token: string;
    apiUrl?: string;
} | {
    name: "cloudflare";
    accountId: string;
    gatewayName: string;
};
type ModelOptions = Omit<LLMClientConfig, "model"> & {
    analytics?: AnalyticsConfig;
};
declare const upstash: (model: UpstashChatModel, options?: Omit<ModelOptions, "baseUrl" | "organization">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
declare const custom: (model: string, options: Omit<ModelOptions, "organization">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
declare const openai: (model: OpenAIChatModel, options?: Omit<ModelOptions, "baseUrl">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
type OllamaModels = (typeof OLLAMA_MODELS)[number] | (string & {});
declare const ollama: (model: OllamaModels, options?: Omit<ModelOptions, "baseUrl"> & {
    port?: number;
}) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
declare const groq: (model: string, options?: Omit<ModelOptions, "baseUrl">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
declare const togetherai: (model: string, options?: Omit<ModelOptions, "baseUrl">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
declare const openrouter: (model: string, options?: Omit<ModelOptions, "baseUrl">) => ChatOpenAI<_langchain_openai.ChatOpenAICallOptions>;
/** Mistral AI does not support any analytics */
declare const mistralai: (model: string, options?: Omit<ModelOptions, "baseUrl">) => ChatMistralAI<_langchain_mistralai.ChatMistralAICallOptions>;
declare const anthropic: (model: string, options?: Omit<ModelOptions, "baseUrl">) => ChatAnthropic;

export { type AddContextOptions, type AddContextPayload, type Branded, type ChatOptions, type CommonChatAndRAGOptions, type CustomPrompt, Database, type DatasWithFileSource, type FilePath, type GetHistoryOptions, type HistoryOptions, HistoryService, InternalUpstashError, type LLMClientConfig, MODEL_NAME_WITH_PROVIDER_SPLITTER, type Message, type OpenAIChatLanguageModel, type OpenAIChatModel, type PrepareChatResult, type ProcessorType, type PromptParameters, RAGChat, type RAGChatConfig, RateLimitService, type RatelimitResponse, RatelimitUpstashError, type ResetOptions, type URL, type UpstashChatModel, type UpstashDict, UpstashError, type UpstashMessage, type VectorPayload, anthropic, custom, groq, mistralai, ollama, openai, openrouter, togetherai, upstash };
